{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89b3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section creates a SparkSession, which is the entry point to programming Spark with the DataFrame and Dataset API. \n",
    "It configures the SparkSession with necessary properties such as application name, stopping gracefully on shutdown, \n",
    "required package, shuffle partitions, and runs Spark in local mode using all available cores. \n",
    "\"\"\"\n",
    "\n",
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession with necessary configurations\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6631ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, necessary PySpark functions and types are imported. \n",
    "The schema is defined for the JSON data that will be read from Kafka. \n",
    "The schema describes the structure of the data, including field names, data types, and nullability.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Define the schema for the JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"rowKey\", StringType(), nullable=False),\n",
    "    StructField(\"click_data\", StructType([\n",
    "        StructField(\"userId\", StringType(), nullable=False),\n",
    "        StructField(\"timestamp\", TimestampType(), nullable=False),\n",
    "        StructField(\"url\", StringType(), nullable=False)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"geo_data\", StructType([\n",
    "        StructField(\"country\", StringType(), nullable=False),\n",
    "        StructField(\"city\", StringType(), nullable=False)\n",
    "    ]), nullable=False),\n",
    "    StructField(\"user_agent_data\", StructType([\n",
    "        StructField(\"browser\", StringType(), nullable=False),\n",
    "        StructField(\"operatingSystem\", StringType(), nullable=False),\n",
    "        StructField(\"device\", StringType(), nullable=False)\n",
    "    ]), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, the code reads streaming data from Kafka. \n",
    "It specifies the Kafka format, bootstrap servers, topic to subscribe to, starting offsets, and loads the data. \n",
    "Then, it selects the value column, casts it to a string, and aliases it as \"value\". \n",
    "Next, it applies the schema to the JSON data using the from_json function and aliases it as \"data\". \n",
    "Finally, it selects all columns from the \"data\" struct.\n",
    "\"\"\"\n",
    "\n",
    "# Read streaming data from Kafka and apply the schema\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"test2\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"value\")) \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, the streaming query is defined to write the processed data to the console. \n",
    "It specifies the output mode as \"append\" (to print only new rows), the output format as \"console\", \n",
    "and the trigger interval as \"20 seconds\" (the time interval to process the data). \n",
    "Finally, the query is started, and the program waits until termination using awaitTermination().\n",
    "\"\"\"\n",
    "\n",
    "# Start the streaming query and process the data\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"20 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63394f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This Section will helps to move the data to Elastic Search. \n",
    "# If we know the elastic search oath we can save it to Elastic Search.\n",
    "\n",
    "\"\"\"\n",
    "In this section, the aggregatedDf DataFrame (which represents the aggregated data) \n",
    "is written to Elasticsearch using the specified configurations.\n",
    "\"\"\"\n",
    "\n",
    "# Set the Elasticsearch index and mapping\n",
    "esConf = {\n",
    "    \"es.nodes\": \"localhost:5601\",\n",
    "    \"es.resource\": \"clickstream\",\n",
    "    \"es.mapping.id\": \"id\",\n",
    "    \"es.mapping.date.rich\": \"false\",\n",
    "    \"es.write.operation\": \"upsert\",\n",
    "    \"es.nodes.wan.only\": \"true\"\n",
    "}\n",
    "\n",
    "# Write the aggregated DataFrame to Elasticsearch\n",
    "aggregatedDf.write \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .options(**esConf) \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
